{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b0e05-0cad-48c5-806b-b5e3d630aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is the KNN algorithm?\n",
    "#Ans-\n",
    "\n",
    "'''The K-Nearest Neighbors (KNN) algorithm is a simple and widely used supervised machine learning algorithm used for classification and regression tasks. It is a non-parametric algorithm, meaning it doesn't make any assumptions about the underlying data distribution. Instead, it relies on the similarity of data points to make predictions.\n",
    "\n",
    "In the KNN algorithm, the \"K\" stands for the number of nearest neighbors to consider when making predictions for a new data point. To classify a new data point, the algorithm looks at the K nearest data points in the training set based on some distance metric (commonly Euclidean distance). \n",
    "The majority class among these K neighbors is then assigned to the new data point in the case of classification tasks. For regression tasks, the algorithm takes the average or weighted average of the K nearest neighbors' target values as the prediction for the new data point.\n",
    "\n",
    "The steps of the KNN algorithm are as follows:\n",
    "\n",
    "1. Choose the number of neighbors (K).\n",
    "2. Calculate the distance between the new data point and all other data points in the training set.\n",
    "3. Select the K-nearest data points based on the calculated distances.\n",
    "4. For classification, assign the class that occurs most frequently among the K neighbors to the new data point. For regression, take the average (or weighted average) of the target values of the K neighbors.\n",
    "5. The new data point is now classified or predicted.\n",
    "\n",
    "KNN is easy to understand and implement, making it a good starting point for many classification and regression problems. \n",
    "However, it can be computationally expensive for large datasets, and its performance can heavily depend on the choice of the distance metric and the value of K. Additionally, it may not work well with high-dimensional data, which is known as the \"curse of dimensionality.\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12fdd09-57e7-49e3-b4ac-f51e1931f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. How do you choose the value of K in KNN?\n",
    "#Ans-\n",
    "\n",
    "'''Choosing the appropriate value of K in the K-Nearest Neighbors (KNN) algorithm is crucial as it can significantly impact the performance of the model. The value of K determines how many neighbors will be considered when making predictions for a new data point. \n",
    "A small K value might result in a noisy decision boundary, leading to overfitting, while a large K value might result in a too smooth decision boundary, leading to underfitting.\n",
    "\n",
    "There is no one-size-fits-all approach to choosing the best K value, and it often requires some experimentation and evaluation. Here are some common methods to help you choose the value of K:\n",
    "\n",
    "1. Cross-Validation: Divide your dataset into training and validation sets. Then, for different values of K, train the KNN model on the training set and evaluate its performance on the validation set using metrics like accuracy, precision, recall, F1-score, etc. Select the K value that gives the best performance on the validation set.\n",
    "\n",
    "2. Odd K Values: Since KNN relies on majority voting, it is better to use odd values of K to avoid ties. Common choices are K=1, 3, 5, 7, and so on.\n",
    "\n",
    "3. Elbow Method: For classification tasks, you can plot the error rate (misclassification rate) against different K values. The plot will typically show a decreasing trend of error rate as K increases. Choose the K value at the \"elbow\" of the curve, where further increases in K don't result in significant decreases in the error rate.\n",
    "\n",
    "4. Domain Knowledge: If you have prior knowledge about the problem domain or the dataset, it might guide you to choose an appropriate value of K. For example, if you know that certain classes are distinct and well-separated, a small K might be sufficient. Conversely, if classes are highly overlapping, a larger K might be necessary.\n",
    "\n",
    "5. Grid Search: If computational resources allow, you can perform an exhaustive search over a range of K values and evaluate the model's performance for each value using cross-validation. Then, select the best K based on the evaluation results.\n",
    "\n",
    "Keep in mind that the best K value might differ for different datasets and problem types. Therefore, it's essential to try multiple approaches and evaluate the model's performance thoroughly to find the optimal K value that generalizes well on unseen data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36116d-ab42-4a84-b543-816ec240e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "#Ans-\n",
    "\n",
    "'''The main difference between the K-Nearest Neighbors (KNN) classifier and KNN regressor lies in the type of machine learning tasks they are used for and the nature of their predictions:\n",
    "\n",
    "1. Task Type:\n",
    "\n",
    "KNN Classifier: KNN is primarily used for classification tasks, where the goal is to assign a categorical label or class to a given input data point. For example, classifying emails as spam or non-spam, identifying the type of a flower based on its features, etc.\n",
    "KNN Regressor: KNN can also be used for regression tasks, where the goal is to predict a continuous numerical value based on the input features. For instance, predicting housing prices based on various features like area, number of bedrooms, etc.\n",
    "\n",
    "2. Prediction Output:\n",
    "\n",
    "KNN Classifier: The KNN classifier predicts the class label for a new data point by considering the majority class among its K-nearest neighbors.\n",
    "KNN Regressor: The KNN regressor predicts the target value for a new data point by taking the average (or weighted average) of the target values of its K-nearest neighbors.\n",
    "\n",
    "3. Distance Metric:\n",
    "\n",
    "Both KNN classifier and regressor use distance metrics (e.g., Euclidean distance, Manhattan distance) to calculate the similarity between data points and determine their nearest neighbors. The choice of distance metric can impact the performance of the algorithm.\n",
    "\n",
    "4. Performance Evaluation:\n",
    "\n",
    "KNN Classifier: The performance of the KNN classifier is typically evaluated using metrics such as accuracy, precision, recall, F1-score, and confusion matrix to assess its classification performance.\n",
    "KNN Regressor: The performance of the KNN regressor is usually evaluated using metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), etc., to measure the accuracy of the continuous predictions.\n",
    "\n",
    "So, KNN classifier and KNN regressor are both variations of the KNN algorithm, but they are used for different types of machine learning tasks and produce different types of predictions. The classifier assigns discrete class labels, while the regressor predicts continuous numerical values.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17f7c4-e954-45dd-a737-adaef7e1def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. How do you measure the performance of KNN?\n",
    "#Ans-\n",
    "\n",
    "'''The performance of the K-Nearest Neighbors (KNN) algorithm can be evaluated using various metrics, depending on whether it is used for classification or regression tasks. Here are the commonly used evaluation metrics for both scenarios:\n",
    "\n",
    "1. Classification (KNN Classifier):\n",
    "In classification tasks, KNN predicts categorical class labels for the input data points. To measure the performance of the KNN classifier, you can use the following metrics:\n",
    "\n",
    "Accuracy: The proportion of correctly classified data points to the total number of data points. It gives an overall measure of the classifier's performance.\n",
    "Precision: The ratio of true positive predictions to the total number of positive predictions. It measures the accuracy of positive predictions made by the classifier.\n",
    "Recall (Sensitivity or True Positive Rate): The ratio of true positive predictions to the total number of actual positive data points. It measures the classifier's ability to identify positive instances.\n",
    "F1-score: The harmonic mean of precision and recall. It balances precision and recall and provides a single score to evaluate the classifier's performance.\n",
    "Confusion Matrix: A table that shows the true positive, true negative, false positive, and false negative predictions of the classifier. It helps to understand the distribution of predictions.\n",
    "\n",
    "2. Regression (KNN Regressor):\n",
    "In regression tasks, KNN predicts continuous numerical values for the input data points. To evaluate the performance of the KNN regressor, you can use the following metrics:\n",
    "\n",
    "Mean Squared Error (MSE): The average of the squared differences between predicted and actual values. It penalizes larger errors more heavily.\n",
    "Root Mean Squared Error (RMSE): The square root of MSE. It is easier to interpret since it is in the same unit as the target variable.\n",
    "Mean Absolute Error (MAE): The average of the absolute differences between predicted and actual values. It is less sensitive to outliers compared to MSE.\n",
    "Cross-Validation:\n",
    "In both classification and regression tasks, it is essential to perform cross-validation to get a more robust estimate of the model's performance. Cross-validation helps prevent overfitting and gives a better representation of how well the model will generalize to unseen data.\n",
    "\n",
    "By using appropriate evaluation metrics and cross-validation techniques, you can assess the performance of the KNN algorithm and fine-tune its parameters, such as the number of neighbors (K) or the distance metric, to improve its effectiveness on real-world data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f871e-8c7d-4300-9156-36deb5328147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. What is the curse of dimensionality in KNN?\n",
    "#Ans-\n",
    "\n",
    "'''The \"curse of dimensionality\" refers to a phenomenon that occurs when working with high-dimensional data in various machine learning algorithms, including the K-Nearest Neighbors (KNN) algorithm. It is a concept that highlights the challenges and limitations of dealing with data in spaces with a large number of dimensions.\n",
    "\n",
    "In high-dimensional spaces:\n",
    "\n",
    "Sparsity of Data: As the number of dimensions increases, the available data becomes sparse. In other words, the data points are more spread out in the higher-dimensional space, and the density of data decreases. This sparsity can make it difficult for KNN to find meaningful neighbors for a given data point, affecting the accuracy of the predictions.\n",
    "\n",
    "Increased Computation: As the number of dimensions increases, the computational cost of calculating distances between data points grows significantly. In KNN, the distance calculation is a key operation, and with high-dimensional data, the process becomes computationally expensive, making the algorithm less efficient.\n",
    "\n",
    "Curse of Choice for K: The choice of the number of neighbors (K) becomes more critical in high-dimensional spaces. With low dimensions, choosing a small K (e.g., K=3 or K=5) might be sufficient to capture local patterns. However, in high dimensions, the concept of \"nearest neighbors\" becomes less informative, and choosing an appropriate K value becomes more challenging.\n",
    "\n",
    "Diminished Separability: High-dimensional data tends to become more uniformly distributed, leading to reduced class separability. In KNN, the algorithm relies on the assumption that similar data points tend to be closer to each other. In high dimensions, this assumption may not hold true, resulting in less effective discrimination between classes.\n",
    "\n",
    "Overfitting: The curse of dimensionality can lead to overfitting, especially when the number of dimensions is much larger than the number of data points. With high-dimensional data, the model may become too complex and fit the noise in the data rather than the underlying patterns, reducing generalization performance.\n",
    "\n",
    "To mitigate the curse of dimensionality in KNN and other high-dimensional data problems, dimensionality reduction techniques like Principal Component Analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE), or feature selection methods can be applied to reduce the number of dimensions and retain the most informative features. \n",
    "Additionally, using feature engineering, domain knowledge, and regularization techniques can help improve the performance of machine learning models in high-dimensional spaces.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903ff69-ccc5-46b5-befe-40ac1f053804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. How do you handle missing values in KNN?\n",
    "#Ans-\n",
    "\n",
    "'''Handling missing values in the K-Nearest Neighbors (KNN) algorithm is essential because missing values can lead to biased and inaccurate distance calculations, which may adversely affect the performance of the algorithm. \n",
    "Here are some common strategies to handle missing values in KNN:\n",
    "\n",
    "Removal of Instances: One straightforward approach is to remove data instances (rows) that contain missing values. However, this method can result in a loss of valuable data, especially if the dataset is small. It should be used with caution when missing values are only present in a small percentage of instances.\n",
    "\n",
    "Imputation with Mean/Median/Mode: For numerical features with missing values, you can replace the missing values with the mean, median, or mode of the available values in the same feature. This way, you maintain the original distribution and central tendency of the data. The choice of imputation method depends on the data and the nature of the missing values.\n",
    "\n",
    "Imputation with KNN: Another approach is to use KNN itself for imputing the missing values. For each missing value, you can use the K-nearest neighbors of that instance and take the average (for numerical features) or majority class (for categorical features) of their values as the imputed value. This method can better preserve local patterns and relationships in the data.\n",
    "\n",
    "Interpolation and Extrapolation: If the dataset has a temporal or spatial structure, you can use interpolation or extrapolation techniques to estimate missing values based on the neighboring values in the sequence or neighboring regions. Methods like linear interpolation, polynomial interpolation, or spline interpolation can be used for this purpose.\n",
    "\n",
    "Regression Models: For missing values in numerical features, you can use regression models to predict the missing values based on other features. You can train a regression model using the instances with complete data and then use it to predict missing values for instances with missing data.\n",
    "\n",
    "Multiple Imputations: In some cases, it may be beneficial to generate multiple imputations for the missing values to account for uncertainty in the imputation process. This can be achieved using techniques like Multiple Imputation by Chained Equations (MICE).\n",
    "\n",
    "It's important to note that the choice of the imputation method can impact the performance of the KNN algorithm and the validity of the results. Careful consideration of the missing data pattern, data distribution, and the underlying problem domain is necessary to choose the most appropriate imputation strategy. \n",
    "Additionally, it is crucial to evaluate the impact of the imputation method on the performance of the KNN algorithm using cross-validation or other evaluation techniques.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe1807-2d1c-43b8-b225-7c0860025458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "#Ans-\n",
    "\n",
    "'''The performance of the K-Nearest Neighbors (KNN) classifier and regressor can vary based on the nature of the problem and the characteristics of the data. Let's compare and contrast their performance and discuss which one might be better suited for different types of problems:\n",
    "\n",
    "KNN Classifier:\n",
    "\n",
    "Performance:\n",
    "\n",
    "The KNN classifier is effective for problems with discrete and categorical class labels.\n",
    "It works well when there are clear boundaries or regions that separate different classes in the feature space.\n",
    "KNN can capture complex decision boundaries, making it suitable for non-linearly separable data.\n",
    "Strengths:\n",
    "\n",
    "Simple to implement and easy to understand.\n",
    "Can handle multi-class classification problems.\n",
    "Robust to noisy data and outliers.\n",
    "Weaknesses:\n",
    "\n",
    "Sensitive to irrelevant or noisy features.\n",
    "Computationally expensive, especially with large datasets and high-dimensional feature spaces.\n",
    "The choice of K value and distance metric can significantly impact the results.\n",
    "Best Suited for:\n",
    "\n",
    "Classification problems, such as image recognition, text classification, sentiment analysis, and medical diagnosis.\n",
    "When the data exhibits well-defined clusters or class separability.\n",
    "KNN Regressor:\n",
    "\n",
    "Performance:\n",
    "\n",
    "The KNN regressor is effective for problems with continuous target variables.\n",
    "It can capture non-linear relationships between features and target values.\n",
    "KNN can handle problems with multiple input features and complex interactions between them.\n",
    "\n",
    "Strengths:\n",
    "Simple to implement and interpret.\n",
    "Versatile and can handle both simple and complex regression tasks.\n",
    "Robust to outliers and noisy data points.\n",
    "\n",
    "Weaknesses:\n",
    "Sensitive to irrelevant features and high-dimensional data.\n",
    "The choice of K value and distance metric can impact the accuracy of predictions.\n",
    "Can be computationally expensive for large datasets.\n",
    "\n",
    "Best Suited for:\n",
    "Regression problems, such as predicting housing prices, stock market prices, or sales forecasts.\n",
    "When the target variable exhibits continuous and smooth relationships with the input features.\n",
    "\n",
    "Which One to Choose:\n",
    "Choose KNN Classifier if your problem involves predicting categorical class labels and the data exhibits well-separated clusters or regions for different classes.\n",
    "Choose KNN Regressor if your problem involves predicting continuous numerical values and the data shows non-linear relationships between features and target values.\n",
    "\n",
    "In practice, it's essential to experiment with both KNN classifier and regressor and compare their performance using appropriate evaluation metrics and cross-validation to determine which one works better for your specific problem.\n",
    "Additionally, consider using dimensionality reduction techniques and preprocessing methods to handle high-dimensional and missing data, which can impact the performance of both KNN classifier and regressor.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a19c2b2-1472-4397-9610-cdc85128b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "#Ans-\n",
    "\n",
    "'''Strengths of KNN Algorithm:\n",
    "\n",
    "Simple and Intuitive: KNN is easy to understand and implement, making it a straightforward algorithm for both classification and regression tasks.\n",
    "\n",
    "Non-parametric: KNN is a non-parametric algorithm, which means it doesn't make assumptions about the underlying data distribution. This flexibility allows it to handle complex data patterns.\n",
    "\n",
    "No Training Phase: Unlike many other algorithms, KNN doesn't require an explicit training phase, as it memorizes the entire dataset. New data points can be classified or predicted immediately.\n",
    "\n",
    "Robust to Outliers: KNN is relatively robust to outliers since it considers the distances to the K-nearest neighbors, and outliers might have a limited impact on the decision.\n",
    "\n",
    "\n",
    "Weaknesses of KNN Algorithm:\n",
    "\n",
    "Computationally Expensive: Calculating distances between data points becomes computationally expensive as the dataset grows, especially for high-dimensional data.\n",
    "\n",
    "Sensitive to Feature Scaling: KNN is sensitive to the scale of features, as features with large scales can dominate the distance calculations. Feature scaling is essential for KNN to avoid biased predictions.\n",
    "\n",
    "Curse of Dimensionality: KNN's performance degrades as the dimensionality of the data increases due to sparsity and increased computation.\n",
    "\n",
    "Choosing K Value: Selecting an appropriate K value is critical. A too small K might result in noise sensitivity, and a too large K might lead to a lack of local patterns.\n",
    "\n",
    "Imbalanced Data: KNN can struggle with imbalanced datasets since the majority class might dominate the predictions.\n",
    "\n",
    "\n",
    "Addressing Weaknesses:\n",
    "\n",
    "Feature Scaling: Normalize or standardize the features to have comparable scales before applying KNN. This can be done by techniques like Min-Max scaling or z-score scaling.\n",
    "\n",
    "Dimensionality Reduction: Use techniques like Principal Component Analysis (PCA) or t-distributed Stochastic Neighbor Embedding (t-SNE) to reduce the dimensionality of the data while retaining essential information.\n",
    "\n",
    "Cross-Validation: Perform cross-validation to obtain more robust estimates of the model's performance and to fine-tune hyperparameters like K.\n",
    "\n",
    "Weighted KNN: Instead of giving each neighbor an equal vote, use weighted KNN, where closer neighbors have higher influence on the predictions.\n",
    "\n",
    "Handling Imbalanced Data: Use techniques like oversampling, undersampling, or class-weighted KNN to address imbalanced datasets.\n",
    "\n",
    "Approximate Nearest Neighbor Algorithms: To handle large datasets efficiently, consider using approximate nearest neighbor algorithms like KD-trees or ball trees.\n",
    "\n",
    "Ensemble Methods: Combine the predictions of multiple KNN models using ensemble methods like bagging or boosting to improve overall performance.\n",
    "\n",
    "By carefully addressing these weaknesses and employing suitable preprocessing techniques, KNN's performance for both classification and regression tasks can be significantly improved. \n",
    "However, it's important to remember that the effectiveness of KNN also heavily depends on the specific characteristics of the dataset and problem domain.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f377b-34d1-416f-a8fa-e521cd11ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "#Ans-\n",
    "\n",
    "'''Euclidean distance and Manhattan distance are two common distance metrics used in the K-Nearest Neighbors (KNN) algorithm to measure the similarity or dissimilarity between data points. Both metrics are used to calculate distances in the feature space and are essential components of the KNN algorithm. \n",
    "The main difference between the two lies in how they measure distances along different axes in the feature space:\n",
    "\n",
    "Euclidean Distance:\n",
    "Euclidean distance is the straight-line distance between two data points in the feature space. It is the most commonly used distance metric and is based on the Pythagorean theorem. The Euclidean distance between two points (x1, y1) and (x2, y2) in a two-dimensional space can be calculated as:\n",
    "\n",
    "Euclidean distance = √((x2 - x1)^2 + (y2 - y1)^2)\n",
    "\n",
    "For higher-dimensional spaces, the Euclidean distance is the generalization of the Pythagorean theorem and is calculated as:\n",
    "\n",
    "Euclidean distance = √(Σ(xi - yi)^2)\n",
    "\n",
    "where (xi, yi) are the coordinates of the two data points along each dimension.\n",
    "\n",
    "Manhattan Distance:\n",
    "Manhattan distance, also known as city block distance or L1 distance, measures the distance between two points by summing the absolute differences between their coordinates along each axis. It is called Manhattan distance because it is the distance a taxi would have to travel along city blocks to reach the destination.\n",
    "\n",
    "The Manhattan distance between two points (x1, y1) and (x2, y2) in a two-dimensional space is calculated as:\n",
    "\n",
    "Manhattan distance = |x2 - x1| + |y2 - y1|\n",
    "\n",
    "For higher-dimensional spaces, the Manhattan distance is calculated as:\n",
    "\n",
    "Manhattan distance = Σ|xi - yi|\n",
    "\n",
    "where (xi, yi) are the coordinates of the two data points along each dimension.\n",
    "\n",
    "Comparison:\n",
    "\n",
    "Euclidean distance calculates the shortest straight-line distance between two points in the feature space, considering all dimensions equally.\n",
    "Manhattan distance calculates the distance by moving along the axes (horizontally and vertically) in the feature space and is not affected by diagonally oriented distances.\n",
    "Euclidean distance tends to give more importance to large differences between coordinates due to the squaring of differences.\n",
    "Manhattan distance is less sensitive to outliers as it only considers absolute differences.\n",
    "The choice of distance metric can impact the performance of the KNN algorithm and may depend on the characteristics of the data and the problem domain.\n",
    "\n",
    "In summary, Euclidean distance and Manhattan distance are two different ways of measuring distances in the feature space, and the choice of distance metric in KNN depends on the specific characteristics of the data and the problem being solved.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc88da-bf16-4740-80b5-70ddc4d3f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10. What is the role of feature scaling in KNN?\n",
    "#Ans-\n",
    "\n",
    "'''Feature scaling plays a crucial role in the K-Nearest Neighbors (KNN) algorithm to ensure that all features contribute equally to the distance calculations between data points. \n",
    "Since KNN relies on measuring distances to determine the nearest neighbors, the scale of features can have a significant impact on the performance of the algorithm. The main reasons for using feature scaling in KNN are as follows:\n",
    "\n",
    "1. Equalizing Feature Contributions: Without feature scaling, features with larger scales or ranges can dominate the distance calculations. \n",
    "For example, if one feature has values in the range of [0, 100] and another feature has values in the range of [0, 1], the first feature's influence on the distance will be much higher. \n",
    "Feature scaling ensures that all features contribute proportionally to the distance calculations, making the algorithm more balanced and fair.\n",
    "\n",
    "2. Improving Convergence: Feature scaling can help the KNN algorithm converge faster during the distance calculation process. \n",
    "When features are at different scales, the distance between data points can take longer to converge to a meaningful value, which can impact the speed of the algorithm.\n",
    "\n",
    "3. Avoiding Bias: In some distance metrics, such as the Euclidean distance, features with larger scales can dominate the overall distance. \n",
    "If certain features are more critical than others, it may lead to a biased representation of the data and potentially impact the KNN's predictive performance.\n",
    "\n",
    "Commonly used techniques for feature scaling in KNN include:\n",
    "\n",
    "Min-Max Scaling (Normalization): Scales the features to a specified range, typically [0, 1] or [-1, 1], using the following formula:\n",
    "\n",
    "x_scaled = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "Z-score Standardization: Scales the features to have a mean of 0 and a standard deviation of 1. It uses the following formula:\n",
    "\n",
    "x_scaled = (x - mean(x)) / std(x)\n",
    "\n",
    "Feature scaling is essential for improving the performance of the KNN algorithm, especially when features have different units or ranges. \n",
    "By scaling the features, KNN can make more accurate distance-based comparisons between data points, leading to better classification and regression results. \n",
    "Additionally, feature scaling is also useful for reducing the computational burden of distance calculations, especially in high-dimensional datasets.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
